---
title: "ktweedie-vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ktweedie-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
set.seed(20220718)
```

# Introduction

`ktweedie` is a package that fits nonparametric Tweedie compound Poisson gamma models in the reproducing kernel Hilbert space. The package is based on two algorithms, the `ktweedie` for kernel-based Tweedie model and the `sktweedie` for sparse kernel-based Tweedie model. The `ktweedie` supports a wide range of kernel functions implemented in the `R` package `kernlab` and the optimization algorithm is a Broyden--Fletcher--Goldfarb--Shanno (BFGS) algorithm with bisection line search. The package includes cross-validation functions for one-dimensional tuning of the kernel regularization parameter $\lambda$ and for two-dimensional joint tuning of one kernel parameter and $\lambda$. The `sktweedie` uses variable weights to achieve variable selection. It is a meta-algorithm that alternatively updates the kernel parameters and a set of variable weights.

The `ktweedie` solves the problem $$ 
\min_{\boldsymbol{\alpha}}\left\{ -\sum_{i=1}^{n}\frac{1}{\phi}\left(\frac{y_{i}e^{(1-\rho)\mathbf{K}_{i}^{\top}\boldsymbol{\alpha}}}{1-\rho}-\frac{e^{(2-\rho)\mathbf{K}_{i}^{\top}\boldsymbol{\alpha}}}{2-\rho}\right)+\lambda\boldsymbol{\alpha}^{\top}\mathbf{K}\boldsymbol{\alpha}\right\} ,
$$ where $\mathbf{K}$ is an $n\times n$ kernel matrix computed according to the user-specified kernel function $K$, whose entries are $K_{ij}=K(\mathbf{x}_i, \mathbf{x}_j)$ are calculated based on the $p$-dimensional predictors from subjects $i,j=1,\ldots,n$. The `sktweedie` solves$$
\begin{aligned}
&\min_{\boldsymbol{\alpha}, \mathbf{w}}\left\{ -\sum_{i=1}^{n}\frac{1}{\phi}\left(\frac{y_{i}e^{(1-\rho)\mathbf{K(w)}_{i}^{\top}\boldsymbol{\alpha}}}{1-\rho}-\frac{e^{(2-\rho)\mathbf{K(w)}_{i}^{\top}\boldsymbol{\alpha}}}{2-\rho}\right)+\lambda_1\boldsymbol{\alpha}^{\top}\mathbf{K(w)}\boldsymbol{\alpha} +\lambda_2 \mathbf{1}^\top \mathbf{w} \right \}\\
& \qquad \qquad \mathrm{s.t.\ \ \ } w_j\in [0,1],\ j=1,\ldots,p,
\end{aligned}$$

where $K(\mathbf{w})_{ij}=K(\mathbf{w \odot x}_i, \mathbf{w \odot x}_j)$ involves variable-specific penalty weights. The methodology and algorithms are described in [cite].

# Installation

From the CRAN.

```{r cran, eval=FALSE}
install.packages("ktweedie")
```

From the Github.

```{r install_git, eval=FALSE}
devtools::install_github("ly129/ktweedie")
```

# Quick Start

First we load the `ktweedie` package:

```{r setup}
library(ktweedie)
```

The package includes a toy data for demonstration purpose.

```{r data, cache = TRUE}
data(dat)
x <- dat$x
y <- dat$y
```

An input matrix `x` and an output vector `y` are now loaded. The `ktd_estimate()` function can be used to fit a basic `ktweedie` model. The regularization parameter `lam1` can be a vector, which will be solved in a decreasing order with warm start.

```{r ktd_estimate1, cache = TRUE}
fit.ktd <- ktd_estimate(x = x, y = y,
                        kern = rbfdot(sigma = 0.1),
                        lam1 = c(0.01, 0.1, 1))
str(fit.ktd$estimates)
```

`fit.ktd$estimates` stores the estimated coefficients and the final objective function value.

The function can also be used to fit the `sktweedie` model:

```{r sktd_est, cache = TRUE}
fit.sktd <- ktd_estimate(x = x, y = y,
                         kern = rbfdot(sigma = 0.1),
                         lam1 = 1,
                         sparsity = TRUE,
                         lam2 = 1)
head(fit.sktd$estimates$`l1 1 l2 1 rbfkernel 0.1`$weight)
```

# Recommended Data Analysis Pipeline

The `ktweedie` and `sktweedie` algorithms require careful tuning of one to multiple hyperparameters, depending on the choice of kernel functions. For the `ktweedie`, we recommend either a one-dimensional tuning for `lam1` ($\lambda_1$) or a two-dimensional random search for `lam1` and the kernel parameter using cross-validation. Tuning is achieved by optimizing a user-specified criterion, including log likelihood "`LL`", mean absolute difference "`MAD`" and root mean squared error "`RMSE`". Using the Laplacian kernel as an example.

```{r laplace-kernel}
laplacedot(sigma = 1)
```

## Cross-validation

The one-dimensional search for the optimal `lam1`, can be achieved with the `ktd_cv()` function from a user-specified vector of candidate values:

```{r one-d-cv, cache = TRUE}
ktd.cv1d <- ktd_cv(x = x, y = y,
                   kern = laplacedot(sigma = 0.01),
                   lambda = c(0.001, 0.01, 0.1, 1, 10),
                   nfolds = 5,
                   loss = "LL")
ktd.cv1d
```

The two-dimensional joint search for the optimal `lam1` and `sigma` requires `ktd_cv2d()`. A total of `ncoefs` pairs of candidate values (uniform random samples on the log scale) are evaluated.

```{r two-d-cv, cache = TRUE}
ktd.cv2d <- ktd_cv2d(x = x, y = y,
                     kernfunc = laplacedot,
                     lambda = c(1e-10, 1e0),
                     sigma = c(1e-10, 1e0),
                     nfolds = 5,
                     ncoefs = 10,
                     loss = "MAD")
ktd.cv2d
```

## Fitting

Then model is fitted using the selected hyperparameter(s):

```{r ktd_fit, cache = TRUE}
ktd.fit <- ktd_estimate(x = x, y = y,
                        kern = laplacedot(sigma = ktd.cv2d$Best_sigma),
                        lam1 = ktd.cv2d$Best_lambda)
str(ktd.fit$estimates)
```

For the `sktweedie`, only the Gaussian radial basis function (RBF) kernel `rbfdot()` is supported. We recommend using the same set of tuned parameters as if a `ktweedie` model is fitted and tuning `lam2` manually:

```{r sktd_fit, cache = TRUE}
sktd.cv2d <- ktd_cv2d(x = x, y = y,
                      kernfunc = rbfdot,
                      lambda = c(1e-3, 1e0),
                      sigma = c(1e-3, 1e0),
                      nfolds = 5,
                      ncoefs = 10,
                      loss = "LL")

sktd.fit <- ktd_estimate(x = x, y = y,
                         kern = rbfdot(sigma = sktd.cv2d$Best_sigma),
                         lam1 = sktd.cv2d$Best_lambda,
                         sparsity = TRUE, lam2 = 1,
                         ftol = 1e-3, partol = 1e-3,
                         innerpartol = 1e-5)
```

## Prediction

The function `ktd_predict()` can identify necessary information stored in `ktd.fit$data` and `sktd.fit$data` to make predictions. If the argument `newdata` is unspecified, the prediction will be made at `x` used in model training and fitting.

```{r fitting, cache = TRUE}
ktd.pred <- ktd_predict(ktd.fit, type = "response")
head(ktd.pred$prediction)
```

If `newdata` with the same dimension as `x` is provided, the prediction will be made at the new data points.

```{r fitting_new, cache = TRUE}
newdata <- x[1:6,]
ktd.pred.new <- ktd_predict(ktd.fit,
                            newdata = newdata,
                            type = "response")
sktd.pred.new <- ktd_predict(sktd.fit,
                             newdata = newdata,
                             type = "response")
data.frame(ktweedie = ktd.pred.new$prediction,
           sktweedie = sktd.pred.new$prediction)
```

## Variable Selection

In practice, the variable selection results of the `sktweedie` is more meaningful. An effective way to fit the `sktweedie` is to start with an arbitrarily big `lam2` that sets all weights to zero and gradually decrease its value.

```{r solution-path, cache = TRUE, fig.height = 8, fig.width = 8}
lam2 <- 25
nlam2 <- 15
wts <- matrix(NA, nrow = nlam2, ncol = ncol(x))
for (i in 1:nlam2) {
  sktd.tmp <- ktd_estimate(x = x, y = y,
                           kern = rbfdot(sigma = sktd.cv2d$Best_sigma),
                           lam1 = sktd.cv2d$Best_lambda,
                           sparsity = TRUE, lam2 = lam2,
                           ftol = 1e-3, partol = 1e-3,
                           innerpartol = 1e-5)
  if (is.null(sktd.tmp$estimates[[1]]$weight)) {
    wts[i, ] <- 0
  } else {
    wts[i, ] <- sktd.tmp$estimates[[1]]$weight
  }
  lam2 <- lam2 * 0.9
}
lam2.seq <- 25 * 0.9^(1:nlam2 - 1)
matplot(y = wts, x = lam2.seq, type = "l",
        log = "x", ylab = "Weights",
        xlab = expression(paste(lambda)),
        lwd = 2)
legend("topright", title = "w index",
       legend = 1:6, lty = 1:5,
       col = 1:6, lwd = 2)
```

## 
